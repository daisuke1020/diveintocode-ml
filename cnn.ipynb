{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畳み込みニューラルネットワーク１"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリを読み込む\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"前回のsprintからクラスを参照\"\"\"\n",
    "class Sigmoid:\n",
    "    \"\"\"シグモイド\"\"\"\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return self.sigmoid(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        _sig = self.sigmoid(self.A)\n",
    "        return dZ * (1 - _sig)*_sig\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"Tanh\"\"\"\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"ソフトマックス\"\"\"\n",
    "    def forward(self, X):\n",
    "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        self.loss = self.loss_func(Y)\n",
    "        return self.Z - Y\n",
    "    \n",
    "    def loss_func(self, Y, Z=None):\n",
    "        if Z is None:\n",
    "            Z = self.Z\n",
    "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"ReLU\"\"\"\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
    "\n",
    "class FC:\n",
    "    \"\"\"全結合\"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        A = X@self.W + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        dZ = dA@self.W.T\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.X.T@dA\n",
    "        self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"sigmoido,tanhの初期化クラス\"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = math.sqrt(1 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "    \n",
    "class HeInitializer():\n",
    "    \"\"\"ReLUの初期化クラス\"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = math.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent : 確率的勾配降下法\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"学習係数を自動で調整してくれる確率的勾配降下法\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 1\n",
    "        self.HB = 1\n",
    "    \n",
    "    def update(self, layer):\n",
    "        self.HW += layer.dW**2\n",
    "        self.HB += layer.dB**2\n",
    "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
    "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB  \n",
    "    \n",
    "class GetMiniBatch:\n",
    "    \"\"\"ミニバッチクラス\"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1] \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"初期化クラス\"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, *shape):\n",
    "        W = self.sigma * np.random.randn(*shape)\n",
    "        return W\n",
    "    \n",
    "    def B(self, *shape):\n",
    "        B = self.sigma * np.random.randn(*shape)\n",
    "        return B\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_i=∑^{F−1}_{s=0} x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "\n",
    "すべてスカラーです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$$\n",
    "w′_s=w_s−α\\frac{\\partial L}{\\partial w_s}\n",
    "$$\n",
    "$$\n",
    "b′=b−α\\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial w_s}=∑^{Nout−1}_{i=0} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}=∑^{Nout−1}_{i=0} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    "\n",
    "\n",
    "$N_{out}$ : 出力のサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j}=∑^{F−1}_{s=0} \\frac{\\partial L}{\\partial a_{(j−s)}}w_s\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "\n",
    "ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差をすべて足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    スクラッチクラス\n",
    "    \"\"\"\n",
    "    \"\"\"フォワード\"\"\"\n",
    "    def forward(self,x,f,b,s=1,p=0):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        --------\n",
    "        x : 入力配列\n",
    "        f : フィルター\n",
    "        b : バイアス\n",
    "        s : ストライド\n",
    "        p : パディング\n",
    "        \"\"\"\n",
    "        h = x.shape[0]\n",
    "        w = len(x)\n",
    "        fs = f.shape[0]\n",
    "        fh = len(f)\n",
    "        oh = (h + 2*p - fh)/s + 1\n",
    "        ow = (w + 2*p - fs)/s + 1\n",
    "\n",
    "        pred =[]\n",
    "        for i in range(int(ow)):\n",
    "            ans = 0\n",
    "            for s in range(int(fs)):\n",
    "                ans += x[i+s] * f[s]\n",
    "            ans = ans + b \n",
    "            print(ans)\n",
    "            pred = np.append(pred,ans)\n",
    "        return pred\n",
    "    \n",
    "    \"\"\"バックプロバケーション\"\"\"\n",
    "    def backward(self,x,f,error_):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        -------\n",
    "        x : 入力配列\n",
    "        f : フィルター\n",
    "        error_ : 誤差\n",
    "        \"\"\"\n",
    "        # バイアスの勾配\n",
    "        delta_b = np.array(np.sum(error_))\n",
    "        \n",
    "        f_ = f[::-1]\n",
    "        a_0 = error_[0]\n",
    "        a_1 = error_[1]\n",
    "        \n",
    "        # 逆伝播の計算\n",
    "        dx3 = a_1 * f[2]        \n",
    "        dx2 = np.sum(error_*f_[0:2])\n",
    "        dx1 = np.sum(error_*f_[1:3])\n",
    "        dx0 = a_0 * f[0]\n",
    "        \n",
    "         # 逆伝播の値\n",
    "        delta_x = np.array([dx0,dx1,dx2,dx3])\n",
    "        \n",
    "        \n",
    "        # \n",
    "        dw2 = np.sum(error_*x[2:4])\n",
    "        dw1 = np.sum(error_*x[1:3])       \n",
    "        dw0 = np.sum(error_*x[0:2])\n",
    "              \n",
    "        # 重みの勾配\n",
    "        delta_w = np.array([dw0,dw1,dw2])\n",
    "        \n",
    "       \n",
    "        \n",
    "        return delta_w,delta_x,delta_b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{out}=\\frac{N_{in}+2P−F}{S}+1\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size_calculation(inp,f,p=0,s=1):\n",
    "    out_ = int((inp + 2*p - f)/s + 1)\n",
    "    \n",
    "    return out_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力x、重みw、バイアスbを次のようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.array([1,2,3,4])\n",
    "f = np.array([3,5,7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションをすると出力は次のようになります。\n",
    "a = np.array([35, 50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バックプロパゲーションをすると次のような値になります。\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "f = np.array([3,5,7])\n",
    "b = np.array([1])\n",
    "s = 1\n",
    "p = 0\n",
    "y = np.array([45,70])\n",
    "error_ =([10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンス化\n",
    "sc_1d = SimpleConv1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35]\n",
      "[50]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 順伝播\n",
    "sc_1d.forward(x,f,b,s,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[ 30 110 170 140]\n",
      "[ 50  80 110]\n"
     ]
    }
   ],
   "source": [
    "# 逆伝播\n",
    "db, dw, dx = sc_1d.backward(x, f, error_)\n",
    "print(dx)\n",
    "print(dw)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\\\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\\\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力は次のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"畳み込みクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0):\n",
    "        \"\"\"コンストラクタ\n",
    "        b_size : フィルターサイズ\n",
    "        initializer : 初期化クラス\n",
    "        optimizer : 最適化手法クラス\n",
    "        n_in_channels : 入力チャンネル数\n",
    "        n_out_channels : 出力チャンネル数\n",
    "        pa : パディング数\n",
    "        \"\"\"\n",
    "        self.b_size = b_size\n",
    "        self.optimizer = optimizer\n",
    "        self.pa = pa\n",
    "        # 重みの初期化\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels) #, b_size)\n",
    "        # バイアスの初期化\n",
    "        self.B = initializer.B(n_out_channels)\n",
    "        self.n_in_channels = n_in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "        self.n_out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        X : 入力配列\n",
    "        \"\"\"\n",
    "        self.X1 =np.zeros((self.n_out_channels ,X.shape[0])).astype(int)\n",
    "        self.s1 =np.zeros((self.n_out_channels ,X.shape[0])).astype(int)\n",
    "        \n",
    "        j = 0\n",
    "        for k in range(self.n_out_channels):\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(self.n_in_channels):\n",
    "                    self.s1 [k,i] +=np.sum(X[i,j:j+3] * self.W[k,i])\n",
    "                \n",
    "        self.s2 = self.s1 + self.B.reshape(-1,1)\n",
    "        \n",
    "        return self.s2\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        dA : 逆伝播してきた配列\n",
    "        \"\"\"\n",
    "        # 重みの勾配\n",
    "        self.dW = np.sum(np.dot(dA, self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa, np.newaxis]), axis=-1)\n",
    "        # バイアスの勾配\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        # 逆伝播の値計算のためにdAを変形\n",
    "        self.dA = np.pad(dA, ((0,0), (0, (self.b_size-1))))\n",
    "        # 出力配列（dX）の計算のためゼロ配列dA1を用意する\n",
    "        self.dA1 = np.zeros((self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
    "        # 重みの長さでループ\n",
    "        for i in range(self.b_size):\n",
    "            self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W@self.dA1, axis=0)\n",
    "        # 重みとバイアスの更新\n",
    "        self.optimizer.update(self)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンス化\n",
    "conc_1d = Conv1d(b_size=3, initializer=SimpleInitializer(0.01), optimizer=SGD(0.01), n_in_channels=2, n_out_channels=3, pa=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
    "# コンストラクタ内で初期化しているけど確認のため再代入\n",
    "conc_1d.W = np.ones((3, 2, 3), dtype=float)\n",
    "conc_1d.B = np.array([1, 2, 3], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conc_1d.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ストライド変更用\"\"\"\n",
    "class Conv1d_Arbitrary_Strides:\n",
    "    \"\"\"畳み込みクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\n",
    "        \"\"\"コンストラクタ\n",
    "        b_size : フィルターサイズ\n",
    "        initializer : 初期化クラス\n",
    "        optimizer : 最適化手法クラス\n",
    "        n_in_channels : 入力チャンネル数\n",
    "        n_out_channels : 出力チャンネル数\n",
    "        pa : パディング数\n",
    "        stride : ストライド数\n",
    "        \"\"\"\n",
    "        self.b_size = b_size\n",
    "        self.optimizer = optimizer\n",
    "        self.pa = pa\n",
    "        self.stride = stride\n",
    "        # 重みの初期化\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
    "        # バイアスの初期化\n",
    "        self.B = initializer.B(n_out_channels)\n",
    "        self.n_in_channels = n_in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "        self.n_out = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        X : 入力配列\n",
    "        \"\"\"\n",
    "        # バッチ数\n",
    "        self.n_samples = X.shape[0]\n",
    "        # 入力配列の特徴量数\n",
    "        self.n_in = X.shape[-1]\n",
    "        # 出力サイズ\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\n",
    "        # 計算のためにX変形\n",
    "        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
    "        # 0埋め実施\n",
    "        self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n",
    "        # 出力配列（A）の計算のためゼロ配列X1を用意する\n",
    "        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
    "        # 重みの長さでループ\n",
    "        for i in range(self.b_size):\n",
    "            # ずらしながら上書き\n",
    "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
    "        # 重みとバイアスを考慮して計算\n",
    "        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        dA : 逆伝播してきた配列\n",
    "        \"\"\"\n",
    "        # 重みの勾配\n",
    "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
    "        # バイアスの勾配\n",
    "        self.dB = np.sum(dA, axis=(0, -1))\n",
    "        # 逆伝播の値計算のためにdAを変形\n",
    "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
    "        # 出力配列（dX）の計算のためゼロ配列dA1を用意する\n",
    "        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
    "        # 重みの長さでループ\n",
    "        for i in range(self.b_size):\n",
    "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
    "        # 重みとバイアスの更新\n",
    "        self.optimizer.update(self)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 画像データ→行データに\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# 訓練データと評価データに\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"スクラッチ畳み込みニューラルネットワーク\"\"\"\n",
    "class ScratchCNNClassifier:\n",
    "    \n",
    "    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=AdaGrad):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        -----------\n",
    "        num_epoch : 学習回数\n",
    "        lr : 学習率\n",
    "        batch_size : バッチサイズ\n",
    "        n_features : 特徴量数\n",
    "        n_nodes1 : 1層目のノード数\n",
    "        n_nodes2 : 2層目のノード数\n",
    "        n_output : 出力層の数\n",
    "        verbose : 仮定出力するか否か\n",
    "        Activater : 活性化関数\n",
    "        Optimizer : 最適化手法\n",
    "        \"\"\"\n",
    "        self.num_epoch = num_epoch\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose  \n",
    "        self.batch_size = batch_size \n",
    "        self.n_features = n_features \n",
    "        self.n_nodes2 = n_nodes2 \n",
    "        self.n_output = n_output \n",
    "        self.Activater = Activater\n",
    "        if Activater == Sigmoid or Activater == Tanh:\n",
    "            self.Initializer = XavierInitializer\n",
    "        elif Activater == ReLU:\n",
    "            self.Initializer = HeInitializer\n",
    "        self.Optimizer = Optimizer\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"学習\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 訓練データの説明変数\n",
    "        y : 訓練データの目的変数\n",
    "        X_val : 評価データの説明変数\n",
    "        y_val : 評価データの目的変数\n",
    "        \"\"\"        \n",
    "        # レイヤー初期化\n",
    "        self.Conv1d_Arbitrary_Strides = Conv1d_Arbitrary_Strides(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n",
    "        self.Conv1d_Arbitrary_Strides.n_out = output_size_calculation(X.shape[-1], self.Conv1d_Arbitrary_Strides.b_size, self.Conv1d_Arbitrary_Strides.pa, self.Conv1d_Arbitrary_Strides.stride)\n",
    "        self.activation1 = self.Activater()\n",
    "        self.FC2 = FC(1*self.Conv1d_Arbitrary_Strides.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\n",
    "        self.activation2 = self.Activater()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\n",
    "        self.activation3 = Softmax()\n",
    "        \n",
    "        # loss配列定義と初期値格納（loss:ミニバッチごとの損失格納  loss_epoch:ミニバッチ学習終了後の全体損失）\n",
    "        self.loss = []\n",
    "        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n",
    "        \n",
    "        # 学習回数分ループ\n",
    "        for _ in range(self.num_epoch):\n",
    "            # ミニバッチイテレータ生成\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            # イテレータ呼び出し\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                # 順伝播\n",
    "                self.forward_propagation(mini_X)\n",
    "                # 逆伝播\n",
    "                self.back_propagation(mini_X, mini_y)\n",
    "                # 損失記録\n",
    "                self.loss.append(self.activation3.loss)\n",
    "            # 損失記録\n",
    "            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"予測値出力\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 説明変数\n",
    "        \"\"\"\n",
    "        return np.argmax(self.forward_propagation(X), axis=1)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"順伝播\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 訓練データの説明変数\n",
    "        \"\"\"\n",
    "        A1 = self.Conv1d_Arbitrary_Strides.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        return Z3\n",
    "        \n",
    "    def back_propagation(self, X, y_true):\n",
    "        \"\"\"逆伝播\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 訓練データの説明変数\n",
    "        y_true : 正解データ\n",
    "        \"\"\"\n",
    "        dA3 = self.activation3.backward(y_true) \n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dA1 = dA1[:, np.newaxis]\n",
    "        dZ0 = self.Conv1d_Arbitrary_Strides.backward(dA1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ScratchCNNClassifier(num_epoch=20, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD)\n",
    "cnn.fit(X_train_[:1000], y_train_[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8997"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cnn.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
